{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\wenji\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Run in python console\n",
    "import nltk; nltk.download('stopwords')\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.models import Phrases\n",
    "from gensim.models import LdaModel\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "from glob import glob\n",
    "\n",
    "# NLTK Stop words\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename='hw3-data/wiki'\n",
    "#contents = open(filename, encoding='utf-8').read()\n",
    "search_path = \"%s/*.txt\" % filename\n",
    "files = glob(search_path)\n",
    "docs=[]\n",
    "for ii in files:\n",
    "    docs.append(open(ii, encoding='utf-8').read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-process and vectorize the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the documents.\n",
    "# Split the documents into tokens.\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "for idx in range(len(docs)):\n",
    "    docs[idx] = docs[idx].lower()  # Convert to lowercase.\n",
    "    docs[idx] = tokenizer.tokenize(docs[idx])  # Split into words.\n",
    "\n",
    "# Remove numbers, but not words that contain numbers.\n",
    "docs = [[token for token in doc if not token.isnumeric()] for doc in docs]\n",
    "\n",
    "# Remove words that are only one character.\n",
    "docs = [[token for token in doc if len(token) > 1] for doc in docs]\n",
    "\n",
    "# Remove words that belong to stopwords\n",
    "docs = [[token for token in doc if token not in stop_words] for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatize the documents.\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "docs = [[lemmatizer.lemmatize(token) for token in doc] for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Bag-of-words representation of the documents.\n",
    "dictionary = Dictionary(docs)\n",
    "corpus = [dictionary.doc2bow(doc) for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tokens: 14391\n",
      "Number of documents: 399\n"
     ]
    }
   ],
   "source": [
    "print('Number of unique tokens: %d' % len(dictionary))\n",
    "print('Number of documents: %d' % len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.780372381210327\n",
      "Total usage of time: 9.78037 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "# Train LDA model.\n",
    "# Set training parameters.\n",
    "num_topics = 5\n",
    "chunksize = 2000\n",
    "passes = 20\n",
    "iterations = 1000\n",
    "eval_every = None  # Don't evaluate model perplexity, takes too much time.\n",
    "\n",
    "# Make a index to word dictionary.\n",
    "temp = dictionary[0]  # This is only to \"load\" the dictionary.\n",
    "id2word = dictionary.id2token\n",
    "\n",
    "start = time.time()\n",
    "model = LdaModel(\n",
    "    corpus=corpus,\n",
    "    id2word=id2word,\n",
    "    chunksize=chunksize,\n",
    "    alpha='auto',\n",
    "    eta='auto',\n",
    "    iterations=iterations,\n",
    "    num_topics=num_topics,\n",
    "    passes=passes,\n",
    "    eval_every=eval_every\n",
    ")\n",
    "total = time.time() - start\n",
    "print(total)\n",
    "print(\"Total usage of time: %0.5f seconds\" % total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average topic coherence: -5.8481.\n",
      "[([(0.0064119156, 'game'),\n",
      "   (0.005138796, 'new'),\n",
      "   (0.004483857, 'year'),\n",
      "   (0.004380243, 'also'),\n",
      "   (0.00404396, 'state'),\n",
      "   (0.0036535512, 'people'),\n",
      "   (0.0035654153, 'team'),\n",
      "   (0.0035473516, 'school'),\n",
      "   (0.003416879, 'monk'),\n",
      "   (0.0033195973, 'one'),\n",
      "   (0.0030732115, 'city'),\n",
      "   (0.003037021, 'first'),\n",
      "   (0.0029875853, 'two'),\n",
      "   (0.0027544051, 'national'),\n",
      "   (0.002618008, 'military'),\n",
      "   (0.002610266, 'season'),\n",
      "   (0.0026097333, 'navy'),\n",
      "   (0.0024629773, 'government'),\n",
      "   (0.002385366, 'yangon'),\n",
      "   (0.002340557, 'time'),\n",
      "   (0.0023199574, 'said'),\n",
      "   (0.0022911683, 'army'),\n",
      "   (0.0022660228, 'football'),\n",
      "   (0.0021490494, 'many'),\n",
      "   (0.0021177197, 'united'),\n",
      "   (0.0020773944, 'high'),\n",
      "   (0.0020630583, 'played'),\n",
      "   (0.002023459, 'report'),\n",
      "   (0.0019917106, 'would'),\n",
      "   (0.0019376725, 'water'),\n",
      "   (0.0018478555, 'temple'),\n",
      "   (0.0018472754, 'junta'),\n",
      "   (0.0018429286, 'station'),\n",
      "   (0.0018353564, 'october'),\n",
      "   (0.0018099904, 'player'),\n",
      "   (0.0018029719, 'day'),\n",
      "   (0.0018024808, 'reported'),\n",
      "   (0.0017941736, 'parish'),\n",
      "   (0.0017832408, 'area'),\n",
      "   (0.0017786983, 'force'),\n",
      "   (0.0017205856, 'york'),\n",
      "   (0.0017128474, 'protest'),\n",
      "   (0.0016681318, 'turkey'),\n",
      "   (0.0016291088, 'american'),\n",
      "   (0.0016000548, 'west'),\n",
      "   (0.0015785211, 'c40'),\n",
      "   (0.0015596219, 'since'),\n",
      "   (0.0015334389, 'myanmar'),\n",
      "   (0.0015301148, 'run'),\n",
      "   (0.0015272482, 'north')],\n",
      "  -4.365141938170418),\n",
      " ([(0.0066470727, 'soil'),\n",
      "   (0.0046678376, 'respiration'),\n",
      "   (0.004463728, 'forest'),\n",
      "   (0.0042392593, 'also'),\n",
      "   (0.0039495197, 'one'),\n",
      "   (0.003890374, 'state'),\n",
      "   (0.0037626105, 'mikoyan'),\n",
      "   (0.0035953019, 'system'),\n",
      "   (0.003072317, 'year'),\n",
      "   (0.002855862, 'time'),\n",
      "   (0.002673106, 'service'),\n",
      "   (0.002485117, 'national'),\n",
      "   (0.0024518173, 'member'),\n",
      "   (0.0022770339, 'first'),\n",
      "   (0.0021526124, 'co2'),\n",
      "   (0.0020872066, 'united'),\n",
      "   (0.0020710386, 'dsrp'),\n",
      "   (0.002070692, 'two'),\n",
      "   (0.002021465, 'city'),\n",
      "   (0.0019899458, 'party'),\n",
      "   (0.0019395051, 'plant'),\n",
      "   (0.0019059908, 'steer'),\n",
      "   (0.0019052258, 'rate'),\n",
      "   (0.0018641792, 'university'),\n",
      "   (0.0017303928, 'used'),\n",
      "   (0.001711389, 'soviet'),\n",
      "   (0.0016576707, 'carbon'),\n",
      "   (0.001617225, 'koretz'),\n",
      "   (0.001617116, 'animal'),\n",
      "   (0.0016100827, 'life'),\n",
      "   (0.0015759089, 'khrushchev'),\n",
      "   (0.0015753126, 'change'),\n",
      "   (0.0015752569, 'root'),\n",
      "   (0.0015708955, 'award'),\n",
      "   (0.0015435229, 'work'),\n",
      "   (0.0015347778, 'structure'),\n",
      "   (0.001526017, 'born'),\n",
      "   (0.001512209, 'would'),\n",
      "   (0.0014607619, 'de'),\n",
      "   (0.0014514027, 'letter'),\n",
      "   (0.0014487897, 'long'),\n",
      "   (0.0014321023, 'district'),\n",
      "   (0.0014230356, 'new'),\n",
      "   (0.001410877, 'romanian'),\n",
      "   (0.0013749269, 'may'),\n",
      "   (0.0013691556, 'increase'),\n",
      "   (0.0013497672, 'well'),\n",
      "   (0.0013400904, 'many'),\n",
      "   (0.001335114, 'several'),\n",
      "   (0.0013311304, 'part')],\n",
      "  -5.353424723243147),\n",
      " ([(0.0047280835, 'one'),\n",
      "   (0.0043921634, 'also'),\n",
      "   (0.003708124, 'city'),\n",
      "   (0.0036255328, 'troop'),\n",
      "   (0.0032814601, 'time'),\n",
      "   (0.002871606, 'first'),\n",
      "   (0.0026947306, 'power'),\n",
      "   (0.0025411614, 'known'),\n",
      "   (0.0022085356, 'year'),\n",
      "   (0.0022028543, 'jack'),\n",
      "   (0.0021838224, 'used'),\n",
      "   (0.002180412, 'part'),\n",
      "   (0.0021477768, 'viterbo'),\n",
      "   (0.0021205756, 'two'),\n",
      "   (0.0021094477, 'war'),\n",
      "   (0.0020384844, 'bar'),\n",
      "   (0.0020224159, 'density'),\n",
      "   (0.0019845543, 'state'),\n",
      "   (0.0019819997, 'system'),\n",
      "   (0.0019283802, 'mram'),\n",
      "   (0.0018186272, 'hector'),\n",
      "   (0.0018158667, 'fish'),\n",
      "   (0.0018143374, 'new'),\n",
      "   (0.0017800408, 'line'),\n",
      "   (0.0017635069, 'temple'),\n",
      "   (0.0017535203, 'many'),\n",
      "   (0.0017530769, 'number'),\n",
      "   (0.0017444437, 'memory'),\n",
      "   (0.0016545906, 'often'),\n",
      "   (0.0016311782, 'well'),\n",
      "   (0.001554761, 'rolla'),\n",
      "   (0.0015452438, 'stewart'),\n",
      "   (0.001545229, 'series'),\n",
      "   (0.0015332815, 'area'),\n",
      "   (0.001525446, 'aircraft'),\n",
      "   (0.0014604321, 'including'),\n",
      "   (0.0014358467, 'layer'),\n",
      "   (0.0014356618, 'micro'),\n",
      "   (0.0014356314, 'machine'),\n",
      "   (0.0014351179, 'became'),\n",
      "   (0.0014350278, 'cell'),\n",
      "   (0.0014193312, 'volume'),\n",
      "   (0.0013808819, 'specie'),\n",
      "   (0.0013808489, 'point'),\n",
      "   (0.0013807251, 'later'),\n",
      "   (0.0013450816, 'water'),\n",
      "   (0.0013259165, 'pope'),\n",
      "   (0.0013258416, 'ram'),\n",
      "   (0.0013253366, 'che'),\n",
      "   (0.0012835393, 'male')],\n",
      "  -6.1491750264658425),\n",
      " ([(0.0068631656, 'nuclear'),\n",
      "   (0.0057087312, 'medicine'),\n",
      "   (0.0049089263, 'camp'),\n",
      "   (0.0042196102, 'also'),\n",
      "   (0.003610554, 'one'),\n",
      "   (0.0027884555, 'first'),\n",
      "   (0.0026856887, 'bullis'),\n",
      "   (0.0026058995, 'used'),\n",
      "   (0.0025075686, 'radiation'),\n",
      "   (0.002256456, 'world'),\n",
      "   (0.002239925, 'imaging'),\n",
      "   (0.002152531, 'line'),\n",
      "   (0.002152153, 'medical'),\n",
      "   (0.0021503698, 'training'),\n",
      "   (0.002062295, 'known'),\n",
      "   (0.0020208715, 'called'),\n",
      "   (0.0018129032, 'unit'),\n",
      "   (0.0017969346, 'many'),\n",
      "   (0.0017962195, 'card'),\n",
      "   (0.0017032982, 'daniel'),\n",
      "   (0.0016678614, 'life'),\n",
      "   (0.0016533122, 'war'),\n",
      "   (0.0016509219, 'patient'),\n",
      "   (0.0016183064, 'process'),\n",
      "   (0.0016098375, 'two'),\n",
      "   (0.0016089029, 'play'),\n",
      "   (0.0016070991, 'new'),\n",
      "   (0.0016033992, 'east'),\n",
      "   (0.0015945965, 'rugby'),\n",
      "   (0.0015800691, 'time'),\n",
      "   (0.001574547, 'may'),\n",
      "   (0.0015344748, 'year'),\n",
      "   (0.0015293638, 'radionuclides'),\n",
      "   (0.0015108363, 'family'),\n",
      "   (0.0014684595, 'village'),\n",
      "   (0.0014404551, 'image'),\n",
      "   (0.001440368, 'sadh'),\n",
      "   (0.0014391901, 'division'),\n",
      "   (0.0014293628, 'university'),\n",
      "   (0.001429098, 'area'),\n",
      "   (0.0013837396, 'game'),\n",
      "   (0.0013569179, 'born'),\n",
      "   (0.0013515623, 'body'),\n",
      "   (0.0013514954, 'scan'),\n",
      "   (0.0013482833, 'part'),\n",
      "   (0.0013425236, 'american'),\n",
      "   (0.0013031879, 'military'),\n",
      "   (0.0012994348, 'end'),\n",
      "   (0.0012627415, 'nfat'),\n",
      "   (0.0012625684, 'ct')],\n",
      "  -6.159195994835981),\n",
      " ([(0.0033331076, 'first'),\n",
      "   (0.0031401676, 'year'),\n",
      "   (0.0028200834, 'leadership'),\n",
      "   (0.0027595805, 'two'),\n",
      "   (0.0026766823, 'one'),\n",
      "   (0.0025770264, 'orie'),\n",
      "   (0.0024082905, 'family'),\n",
      "   (0.0022967863, 'culture'),\n",
      "   (0.0022876712, 'station'),\n",
      "   (0.0022470201, 'cargo'),\n",
      "   (0.0022402192, 'cable'),\n",
      "   (0.0022313248, 'also'),\n",
      "   (0.0021644048, 'pommern'),\n",
      "   (0.0021633648, 'transformational'),\n",
      "   (0.002142167, 'may'),\n",
      "   (0.0020717368, 'village'),\n",
      "   (0.002062476, 'age'),\n",
      "   (0.0019702502, 'work'),\n",
      "   (0.001868169, 'south'),\n",
      "   (0.0018465006, 'later'),\n",
      "   (0.0018341135, 'leader'),\n",
      "   (0.0018339355, 'british'),\n",
      "   (0.0018337392, 'film'),\n",
      "   (0.0018227686, 'born'),\n",
      "   (0.001818375, 'made'),\n",
      "   (0.0017513818, 'method'),\n",
      "   (0.0017467887, 'member'),\n",
      "   (0.0017439304, 'ship'),\n",
      "   (0.001668711, 'fleet'),\n",
      "   (0.0016685057, 'bacon'),\n",
      "   (0.0016683633, 'syndrome'),\n",
      "   (0.0016599466, 'household'),\n",
      "   (0.0016315328, 'new'),\n",
      "   (0.0015859471, 'beresford'),\n",
      "   (0.0015854598, 'follower'),\n",
      "   (0.0015851747, 'ramsay'),\n",
      "   (0.0015523685, 'service'),\n",
      "   (0.0015219084, 'day'),\n",
      "   (0.0015211771, 'including'),\n",
      "   (0.0015177247, 'american'),\n",
      "   (0.0015034422, 'roman'),\n",
      "   (0.0014560312, 'state'),\n",
      "   (0.0014389, 'part'),\n",
      "   (0.0014207893, 'church'),\n",
      "   (0.0014207141, 'wombles'),\n",
      "   (0.0013469926, 'history'),\n",
      "   (0.001344627, 'child'),\n",
      "   (0.0013384186, 'group'),\n",
      "   (0.0013384022, 'world'),\n",
      "   (0.0013382548, 'sea')],\n",
      "  -7.213725676305459)]\n"
     ]
    }
   ],
   "source": [
    "top_topics = model.top_topics(corpus, topn=50)#, num_words=20)\n",
    "\n",
    "# Average topic coherence is the sum of topic coherences of all topics, divided by the number of topics.\n",
    "avg_topic_coherence = sum([t[1] for t in top_topics]) / num_topics\n",
    "print('Average topic coherence: %.4f.' % avg_topic_coherence)\n",
    "\n",
    "from pprint import pprint\n",
    "pprint(top_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "topicsfile = open(\"gensim.txt\", 'w')\n",
    "for t in range(len(top_topics)):\n",
    "    i,j = top_topics[t]\n",
    "    topicsfile.write(\"------------\\nTopic %i (%0.5f)\\n------------\\n\" % (t, j))\n",
    "    for k,l in i:\n",
    "        topicsfile.write('%0.5f\\t%s\\n' % (k,l))\n",
    "topicsfile.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
